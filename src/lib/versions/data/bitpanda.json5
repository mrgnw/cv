{
  "skills": [
    "Yo mama",
    "Python",
    "SQL",
    "ETL",
    "AWS",
    "Airflow",
    "Databricks",
    "PostgreSQL",
    "APIs"
  ],
  "secondarySkills": [
    "DuckDB",
    "SQLMesh",
    "GoLang",
    "FastAPI",
    "JavaScript",
    "Svelte"
  ],
  "experience": [
  {
    "title": "Data Engineering Consultant",
        "company": "CGI",
        start: "2025-03-17",
        "skills": ["Python", "AWS", "ETL", "SQL", "CI/CD", "Databricks", "Airflow"],
        "achievements":[
				"Deployed custom AWS Lambda connector, coordinating with other teams to add a new integration enabling a new subscription type, while also ensuring that there was 0 downtime for the billions of other streaming transactions happening with our other clients.",
				"Accelerated security vulnerability remediation from days to minutes by migrating 5 major projects to UV and Ruff, enabling rapid dependency updates and automated code quality checks while reducing package installation time by 10-100x and cutting CI/CD pipeline runtime by 60%.",
				"Provided ongoing production support and troubleshooting for data pipeline issues across 5+ enterprise client projects, leveraging CloudWatch monitoring and logging",
				// "Designed and implemented CI/CD pipelines using GitLab Pipelines (pytest, unittest), .",
				// "Developed RESTful APIs using Python, FastAPI, and Flask with comprehensive OpenAPI documentation, empowering 50+ business users to programmatically access company data through secure endpoints, reducing manual reporting requests by 60% and improving data accessibility.",
				// "Modernized legacy ETL workflows to cloud-native architectures using Apache Airflow, Databricks, and AWS services (Lambda, S3, EC2, Glue), accelerating batch processing tasks by 100x-1000x through parallelization, automation, and serverless computing patterns.",
        ]
    },
    {
      "title": "Data Engineering Consultant",
      "company": "National Care Dental",
      "start": "2022-05-01",
      "achievements": [
        "Designed and implemented scalable data infrastructure and pipelines using AWS and Python, enabling faster business operations and enhanced data insights while cutting costs significantly",
        "Transitioned legacy workflows to modern data pipelines with automated testing and monitoring, accelerating tasks by 100x to 1000x through automation and reducing multi-hour processes to seconds",
        "Mentored data analysts in Python and SQL best practices, expanding their technical capabilities and establishing data quality standards and governance processes",
        "Built robust, observable data pipelines with comprehensive documentation and clear lifecycle standards, ensuring performance and reliability across all data operations"
      ],
      "skills": ["Python", "AWS", "ETL", "SQL", "APIs", "Data Governance"]
    },
    {
      "title": "Senior Data Engineer",
      "company": "Persefoni",
      "start": "2021-08-01",
      "end": "2022-05-31",
      "achievements": [
        "Implemented production-grade ETL pipelines in Databricks using Python and SQL for a rapidly growing climate-tech startup, applying solid engineering practices including testing and CI/CD",
        "Created scalable API endpoints in GoLang, collaborating across product and engineering teams to integrate multiple services and support business requirements",
        // "Documented and established new processes for data pipeline operations, improving team onboarding and increasing overall productivity through knowledge sharing",
        // "Partnered with stakeholders across operations and finance to turn business goals into measurable data solutions using modern cloud infrastructure"
      ],
      "skills": ["Python", "Databricks", "ETL", "SQL", "GoLang", "APIs"]
    },
    {
      "title": "Software Engineer",
      "company": "Zelis Healthcare",
      "start": "2018-12-01",
      "end": "2021-08-01",
      "achievements": [
        "Created and optimized ETL workflows in Airflow, improving data processing performance by up to 1000x through efficient pipeline design and implementation",
        "Migrated multiple legacy codebases to Python3, enhancing maintainability and establishing modern engineering practices including code reviews and version control",
        //"Developed Python APIs and internal web interfaces using Flask, empowering business users across teams to meaningfully interact with company data and analytics",
        // "Built data transformation pipelines that supported advanced analytics and reporting requirements for healthcare data processing"
      ],
      "skills": ["Python", "Airflow", "Flask", "ETL", "Data Modeling"]
    }
  ],
  "keywords": [
    "Data Engineering",
    "Python",
    "SQL",
    "ETL",
    "AWS",
    "Airflow",
    "Databricks",
    "Spark",
    "Data Pipelines",
    "Data Governance",
    "Data Quality",
    "Snowflake",
    "dbt",
    "Terraform",
    "CI/CD",
    "Data Modeling",
    "Analytics",
    "Business Intelligence",
    "Cloud Computing",
    "Performance Optimization",
    "Data Warehousing",
    "APIs",
    "Kubernetes",
    "Data Validation",
    "Documentation",
    "Stakeholder Collaboration",
    "Financial Data",
    "Healthcare Data",
    "Climate Data"
  ],
  "company": "Bitpanda",
  "title": "Data Engineer",
  "normalizedTitle": "data",
  "matchScore": 8,
  "matchFactors": {
    "positives": [
      "Strong Python and SQL experience with production ETL pipelines",
      "Extensive AWS and Airflow experience matching tech stack requirements",
      "Data governance and quality experience aligns with role requirements",
      "Cross-functional stakeholder collaboration experience",
      "Mentoring and documentation skills for team contribution"
    ],
    "negatives": [
      "No direct Snowflake experience mentioned",
      "Limited dbt and Dagster experience",
      "No specific Terraform infrastructure-as-code experience listed"
    ],
    "scoreBreakdown": {
      "techStack": 3,
      "role": 2,
      "culture": 2,
      "domain": 1,
      "constraints": 0
    },
    "summary": "Excellent match with strong data engineering fundamentals, AWS/Python expertise, and stakeholder collaboration skills. Some gaps in specific tools like Snowflake and dbt."
  },
  "projects": [
    "Resume",
    "Multilingual translator",
    // "TextMe",
  ]

}
